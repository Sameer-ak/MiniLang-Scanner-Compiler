{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KByZ5YzvwZlU",
        "outputId": "88dbe7a3-a892-4ea8-c8b2-69204d0f4827"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KEYWORD if\n",
            "ERROR (\n",
            "IDENTIFIER x\n",
            "OPERATOR =\n",
            "OPERATOR =\n",
            "INTEGER 10\n",
            "ERROR )\n",
            "ERROR {\n",
            "KEYWORD print\n",
            "KEYWORD true\n",
            "ERROR ;\n",
            "ERROR }\n",
            "KEYWORD else\n",
            "ERROR {\n",
            "KEYWORD print\n",
            "KEYWORD false\n",
            "ERROR ;\n",
            "ERROR }\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "class TokenType:\n",
        "    INTEGER = 'INTEGER'\n",
        "    BOOLEAN = 'BOOLEAN'\n",
        "    OPERATOR = 'OPERATOR'\n",
        "    ASSIGNMENT = 'ASSIGNMENT'\n",
        "    EQUALITY = 'EQUALITY'\n",
        "    INEQUALITY = 'INEQUALITY'\n",
        "    KEYWORD = 'KEYWORD'\n",
        "    IDENTIFIER = 'IDENTIFIER'\n",
        "    PRINT = 'PRINT'\n",
        "    TRUE = 'TRUE'\n",
        "    FALSE = 'FALSE'\n",
        "    COMMENT = 'COMMENT'\n",
        "    ERROR = 'ERROR'\n",
        "\n",
        "class Token:\n",
        "    def __init__(self, type, lexeme):\n",
        "        self.type = type\n",
        "        self.lexeme = lexeme\n",
        "\n",
        "def scan_file(filename):\n",
        "    keywords = {'if', 'else', 'print', 'true', 'false'}\n",
        "    operators = {'+', '-', '*', '/', '=', '==', '!='}\n",
        "\n",
        "    tokens = []\n",
        "    with open(filename, 'r') as file:\n",
        "        for line_number, line in enumerate(file):\n",
        "            line = line.strip()\n",
        "            while '//' in line:\n",
        "                comment_start = line.index('//')\n",
        "                line = line[:comment_start]\n",
        "\n",
        "            if not line:\n",
        "                continue\n",
        "\n",
        "            i = 0\n",
        "            while i < len(line):\n",
        "                if line[i].isdigit():\n",
        "                    match = re.match(r'\\d+', line[i:])\n",
        "                    if match:\n",
        "                        tokens.append(Token(TokenType.INTEGER, match.group(0)))\n",
        "                        i += len(match.group(0))\n",
        "                    else:\n",
        "                        tokens.append(Token(TokenType.ERROR, line[i]))\n",
        "                        i += 1\n",
        "                elif line[i].isalpha() or line[i] == '_':\n",
        "                    match = re.match(r'[a-zA-Z_]\\w*', line[i:])\n",
        "                    if match:\n",
        "                        lexeme = match.group(0)\n",
        "                        if lexeme in keywords:\n",
        "                            tokens.append(Token(TokenType.KEYWORD, lexeme))\n",
        "                        elif lexeme == 'true':\n",
        "                            tokens.append(Token(TokenType.BOOLEAN, lexeme))\n",
        "                        elif lexeme == 'false':\n",
        "                            tokens.append(Token(TokenType.BOOLEAN, lexeme))\n",
        "                        else:\n",
        "                            tokens.append(Token(TokenType.IDENTIFIER, lexeme))\n",
        "                        i += len(match.group(0))\n",
        "                    else:\n",
        "                        tokens.append(Token(TokenType.ERROR, line[i]))\n",
        "                        i += 1\n",
        "                elif line[i] in operators:\n",
        "                    tokens.append(Token(TokenType.OPERATOR, line[i]))\n",
        "                    i += 1\n",
        "                elif line[i:i+2] in ('==', '!='):\n",
        "                    tokens.append(Token(TokenType.EQUALITY if line[i:i+2] == '==' else TokenType.INEQUALITY, line[i:i+2]))\n",
        "                    i += 2\n",
        "                elif line[i] == '=':\n",
        "                    tokens.append(Token(TokenType.ASSIGNMENT, line[i]))\n",
        "                    i += 1\n",
        "                elif line[i] == ' ':\n",
        "                    i += 1\n",
        "                elif line[i] == '\\t':\n",
        "                    i += 1\n",
        "                elif line[i] == '\\r':\n",
        "                    i += 1\n",
        "                elif line[i] == '\\n':\n",
        "                    i += 1\n",
        "                else:\n",
        "                    tokens.append(Token(TokenType.ERROR, line[i]))\n",
        "                    i += 1\n",
        "\n",
        "    return tokens\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    filename =  \"tc_2.minilang\"\n",
        "    tokens = scan_file(filename)\n",
        "    for token in tokens:\n",
        "        print(token.type, token.lexeme)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5CuB0okAz0Yp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wPEL4NiLz6hT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zsN4-uusxsXV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}